{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emotic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOvEGajUxIXQTcOluZM1wCA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tandon-A/emotic/blob/master/Colab_train_emotic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5Xan2tnR89K",
        "colab_type": "text"
      },
      "source": [
        "<h1><center> Emotions in context (Emotic) </center></h1>\n",
        "<center> Using context information to recognize emotions in images</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rbCWI0rkt8yp"
      },
      "source": [
        "<h1>Project context</h1>\n",
        "\n",
        "Humans use their facial features or expressions to convey how they feel, such as a person may smile when happy and scowl when angry. Historically, computer vision research has focussed on analyzing and learning these facial features to recognize emotions. \n",
        "However, these facial features are not universal and vary extensively across cultures and situations. \n",
        "\n",
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/Tandon-A/emotic/master/assets/face.jpg\"> <img src=\"https://raw.githubusercontent.com/Tandon-A/emotic/master/assets/full_scene.jpg\" width=\"400\">\n",
        "  <figcaption>Fig 1: a) (Facial feature) The person looks angry or in pain b) (Whole scene) The person looks elated.</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "A scene context, as shown in the figure above, can provide additional information about the situations. This project explores the use of context in recognizing emotions in images. \n",
        "\n",
        "This project uses the <a href=\"http://sunai.uoc.edu/emotic/download.html\">EMOTIC dataset</a> and follows the methodology as introduced in the paper <a href=\"https://arxiv.org/pdf/2003.13401.pdf\">'Context based emotion recognition using EMOTIC dataset'</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YFaW8HlNWnE",
        "colab_type": "code",
        "outputId": "d0ce63cc-8a6c-463b-88a5-ece54990eb09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Linking Google drive to use preprocessed data \n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "#/content/drive/My Drive//"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhzX7KUihZqu",
        "colab_type": "text"
      },
      "source": [
        "# I. Prepare places pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYgeeri3wdCM",
        "colab_type": "code",
        "outputId": "e7600276-d296-4d16-dad0-7d7293210466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "# Get Resnet18 model trained on places dataset. \n",
        "!mkdir ./places\n",
        "!wget http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar -O ./places/resnet18_places365.pth.tar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./places’: File exists\n",
            "--2020-05-23 16:56:44--  http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar\n",
            "Resolving places2.csail.mit.edu (places2.csail.mit.edu)... 128.30.100.255\n",
            "Connecting to places2.csail.mit.edu (places2.csail.mit.edu)|128.30.100.255|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45506139 (43M) [application/x-tar]\n",
            "Saving to: ‘./places/resnet18_places365.pth.tar’\n",
            "\n",
            "./places/resnet18_p 100%[===================>]  43.40M  22.7MB/s    in 1.9s    \n",
            "\n",
            "2020-05-23 16:56:47 (22.7 MB/s) - ‘./places/resnet18_places365.pth.tar’ saved [45506139/45506139]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyigpTRvws6W",
        "colab_type": "code",
        "outputId": "8b98e435-75e7-40c5-c7ec-9922fd0d63c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Converting model weights to python3.6 format\n",
        "import torch\n",
        "from torch.autograd import Variable as V\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms as trn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "\n",
        "model_path = './places'\n",
        "archs = ['resnet18']\n",
        "for arch in archs:\n",
        "    model_file = os.path.join(model_path,'%s_places365.pth.tar' % arch)\n",
        "    save_file = os.path.join(model_path,'%s_places365_py36.pth.tar' % arch)\n",
        "\n",
        "    from functools import partial\n",
        "    import pickle\n",
        "    pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
        "    pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
        "    model = torch.load(model_file, map_location=lambda storage, loc: storage, pickle_module=pickle)\n",
        "    torch.save(model, save_file)\n",
        "    print('converting %s -> %s'%(model_file, save_file))\n",
        "\n",
        "print ('completed cell')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "converting ./places/resnet18_places365.pth.tar -> ./places/resnet18_places365_py36.pth.tar\n",
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhWL6Qi_w4qp",
        "colab_type": "code",
        "outputId": "dd6271c6-54c4-4f56-ddd9-b83eb0f606ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Saving the model weights to use ahead in the notebook\n",
        "import torch\n",
        "from torch.autograd import Variable as V\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "from torchvision import transforms as trn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "\n",
        "# the architecture to use\n",
        "arch = 'resnet18'\n",
        "model_weight = os.path.join(model_path, 'resnet18_places365_py36.pth.tar')\n",
        "\n",
        "# create the network architecture\n",
        "model = models.__dict__[arch](num_classes=365)\n",
        "\n",
        "#model_weight = '%s_places365.pth.tar' % arch\n",
        "\n",
        "checkpoint = torch.load(model_weight, map_location=lambda storage, loc: storage) # model trained in GPU could be deployed in CPU machine like this!\n",
        "state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()} # the data parallel layer will add 'module' before each layer name\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "model.cpu()\n",
        "torch.save(model, os.path.join(model_path, 'res_context' + '.pth'))\n",
        "print ('completed cell')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykNjfrUuhpbq",
        "colab_type": "text"
      },
      "source": [
        "# II. General imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi-O8QgwvOQY",
        "colab_type": "code",
        "outputId": "b08b8a76-2a53-4d3c-fd66-8b0a838c8054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import scipy.io\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim \n",
        "from torch.utils.data import Dataset, DataLoader \n",
        "from torchsummary import summary\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "print ('completed cell')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD0pBBBYh2vW",
        "colab_type": "text"
      },
      "source": [
        "# III. Emotic classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfPKerg4TWkR",
        "colab_type": "text"
      },
      "source": [
        "## Emotic Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWt88EcJVu0c",
        "colab_type": "code",
        "outputId": "45c7c38e-1491-47cf-d4c8-a1d88d0d59a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class Emotic(nn.Module):\n",
        "  ''' Emotic Model'''\n",
        "  def __init__(self, num_context_features, num_body_features):\n",
        "    super(Emotic,self).__init__()\n",
        "    self.num_context_features = num_context_features\n",
        "    self.num_body_features = num_body_features\n",
        "    self.fc1 = nn.Linear((self.num_context_features + num_body_features), 256)\n",
        "    self.bn1 = nn.BatchNorm1d(256)\n",
        "    self.d1 = nn.Dropout(p=0.5)\n",
        "    self.fc_cat = nn.Linear(256, 26)\n",
        "    self.fc_cont = nn.Linear(256, 3)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    \n",
        "  def forward(self, x_context, x_body):\n",
        "    context_features = x_context.view(-1, self.num_context_features)\n",
        "    body_features = x_body.view(-1, self.num_body_features)\n",
        "    fuse_features = torch.cat((context_features, body_features), 1)\n",
        "    fuse_out = self.fc1(fuse_features)\n",
        "    fuse_out = self.bn1(fuse_out)\n",
        "    fuse_out = self.relu(fuse_out)\n",
        "    fuse_out = self.d1(fuse_out)    \n",
        "    cat_out = self.fc_cat(fuse_out)\n",
        "    cont_out = self.fc_cont(fuse_out)\n",
        "    return cat_out, cont_out\n",
        "\n",
        "print ('completed cell')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdzZGj6AxLaC",
        "colab_type": "text"
      },
      "source": [
        "## Emotic Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKG5dNMXxlnm",
        "colab_type": "code",
        "outputId": "f946e2dc-921d-4c8c-bfb9-b05129e90e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class Emotic_PreDataset(Dataset):\n",
        "  ''' Custom Emotic dataset class. Use preprocessed data stored in npy files. '''\n",
        "  def __init__(self, x_context, x_body, y_cat, y_cont, transform, context_norm, body_norm):\n",
        "    super(Emotic_PreDataset,self).__init__()\n",
        "    self.x_context = x_context\n",
        "    self.x_body = x_body\n",
        "    self.y_cat = y_cat \n",
        "    self.y_cont = y_cont\n",
        "    self.transform = transform \n",
        "    self.context_norm = transforms.Normalize(context_norm[0], context_norm[1])  # Normalizing the context image with context mean and context std\n",
        "    self.body_norm = transforms.Normalize(body_norm[0], body_norm[1])           # Normalizing the body image with body mean and body std\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y_cat)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    image_context = self.x_context[index]\n",
        "    image_body = self.x_body[index]\n",
        "    cat_label = self.y_cat[index]\n",
        "    cont_label = self.y_cont[index]\n",
        "    return self.context_norm(self.transform(image_context)), self.body_norm(self.transform(image_body)), torch.tensor(cat_label, dtype=torch.float32), torch.tensor(cont_label, dtype=torch.float32)/10.0\n",
        "\n",
        "print ('completed cell')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFuEQruAxQrK",
        "colab_type": "text"
      },
      "source": [
        "## Emotic Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObffJVXkqsJg",
        "colab_type": "code",
        "outputId": "19050d6b-6dbd-45c3-8c40-d54bb834c497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class DiscreteLoss(nn.Module):\n",
        "  ''' Class to measure loss between categorical emotion predictions and labels.'''\n",
        "  def __init__(self, weight_type='mean', device=torch.device('cpu')):\n",
        "    super(DiscreteLoss, self).__init__()\n",
        "    self.weight_type = weight_type\n",
        "    self.device = device\n",
        "    if self.weight_type == 'mean':\n",
        "      self.weights = torch.ones((1,26))/26.0\n",
        "      self.weights = self.weights.to(self.device)\n",
        "    elif self.weight_type == 'static':\n",
        "      self.weights = torch.FloatTensor([0.1435, 0.1870, 0.1692, 0.1165, 0.1949, 0.1204, 0.1728, 0.1372, 0.1620,\n",
        "         0.1540, 0.1987, 0.1057, 0.1482, 0.1192, 0.1590, 0.1929, 0.1158, 0.1907,\n",
        "         0.1345, 0.1307, 0.1665, 0.1698, 0.1797, 0.1657, 0.1520, 0.1537]).unsqueeze(0)\n",
        "      self.weights = self.weights.to(self.device)\n",
        "    \n",
        "  def forward(self, pred, target):\n",
        "    if self.weight_type == 'dynamic':\n",
        "      self.weights = self.prepare_dynamic_weights(target)\n",
        "      self.weights = self.weights.to(self.device)\n",
        "    loss = (((pred - target)**2) * self.weights)\n",
        "    return loss.sum() \n",
        "\n",
        "  def prepare_dynamic_weights(self, target):\n",
        "    target_stats = torch.sum(target, dim=0).float().unsqueeze(dim=0).cpu()\n",
        "    weights = torch.zeros((1,26))\n",
        "    weights[target_stats != 0 ] = 1.0/torch.log(target_stats[target_stats != 0].data + 1.2)\n",
        "    weights[target_stats == 0] = 0.0001\n",
        "    return weights\n",
        "\n",
        "\n",
        "class ContinuousLoss_L2(nn.Module):\n",
        "  ''' Class to measure loss between continuous emotion dimension predictions and labels. Using l2 loss as base. '''\n",
        "  def __init__(self, margin=1):\n",
        "    super(ContinuousLoss_L2, self).__init__()\n",
        "    self.margin = margin\n",
        "  \n",
        "  def forward(self, pred, target):\n",
        "    labs = torch.abs(pred - target)\n",
        "    loss = labs ** 2 \n",
        "    loss[ (labs < self.margin) ] = 0.0\n",
        "    return loss.sum()\n",
        "\n",
        "\n",
        "class ContinuousLoss_SL1(nn.Module):\n",
        "  ''' Class to measure loss between continuous emotion dimension predictions and labels. Using smooth l1 loss as base. '''\n",
        "  def __init__(self, margin=1):\n",
        "    super(ContinuousLoss_SL1, self).__init__()\n",
        "    self.margin = margin\n",
        "  \n",
        "  def forward(self, pred, target):\n",
        "    labs = torch.abs(pred - target)\n",
        "    loss = 0.5 * (labs ** 2)\n",
        "    loss[ (labs > self.margin) ] = labs[ (labs > self.margin) ] - 0.5\n",
        "    return loss.sum()\n",
        "\n",
        "print ('completed cell')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AMUYcy5h9cM",
        "colab_type": "text"
      },
      "source": [
        "# IV. Load preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSadne_Bc5va",
        "colab_type": "code",
        "outputId": "20993809-6f85-4fa9-846b-c9fa90f8d7fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Change data_src variable as per your drive\n",
        "data_src = '/content/drive/My Drive/Colab/Emotic/data'\n",
        "\n",
        "\n",
        "# Load training preprocessed data\n",
        "train_context = np.load(os.path.join(data_src,'pre','train_context_arr.npy'))\n",
        "train_body = np.load(os.path.join(data_src,'pre','train_body_arr.npy'))\n",
        "train_cat = np.load(os.path.join(data_src,'pre','train_cat_arr.npy'))\n",
        "train_cont = np.load(os.path.join(data_src,'pre','train_cont_arr.npy'))\n",
        "\n",
        "# Load validation preprocessed data \n",
        "val_context = np.load(os.path.join(data_src,'pre','val_context_arr.npy'))\n",
        "val_body = np.load(os.path.join(data_src,'pre','val_body_arr.npy'))\n",
        "val_cat = np.load(os.path.join(data_src,'pre','val_cat_arr.npy'))\n",
        "val_cont = np.load(os.path.join(data_src,'pre','val_cont_arr.npy'))\n",
        "\n",
        "# Load testing preprocessed data\n",
        "test_context = np.load(os.path.join(data_src,'pre','test_context_arr.npy'))\n",
        "test_body = np.load(os.path.join(data_src,'pre','test_body_arr.npy'))\n",
        "test_cat = np.load(os.path.join(data_src,'pre','test_cat_arr.npy'))\n",
        "test_cont = np.load(os.path.join(data_src,'pre','test_cont_arr.npy'))\n",
        "\n",
        "# Categorical emotion classes\n",
        "cat = ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion', 'Confidence', 'Disapproval', 'Disconnection',\n",
        "       'Disquietment', 'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem', 'Excitement', 'Fatigue', 'Fear',\n",
        "       'Happiness', 'Pain', 'Peace', 'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise', 'Sympathy', 'Yearning']\n",
        "\n",
        "cat2ind = {}\n",
        "ind2cat = {}\n",
        "for idx, emotion in enumerate(cat):\n",
        "  cat2ind[emotion] = idx\n",
        "  ind2cat[idx] = emotion\n",
        "\n",
        "print ('train ', 'context ', train_context.shape, 'body', train_body.shape, 'cat ', train_cat.shape, 'cont', train_cont.shape)\n",
        "print ('val ', 'context ', val_context.shape, 'body', val_body.shape, 'cat ', val_cat.shape, 'cont', val_cont.shape)\n",
        "print ('test ', 'context ', test_context.shape, 'body', test_body.shape, 'cat ', test_cat.shape, 'cont', test_cont.shape)\n",
        "print ('completed cell')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train  context  (23266, 224, 224, 3) body (23266, 128, 128, 3) cat  (23266, 26) cont (23266, 3)\n",
            "val  context  (3315, 224, 224, 3) body (3315, 128, 128, 3) cat  (3315, 26) cont (3315, 3)\n",
            "test  context  (7203, 224, 224, 3) body (7203, 128, 128, 3) cat  (7203, 26) cont (7203, 3)\n",
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JySFyUFZNgPy",
        "colab_type": "code",
        "outputId": "7a8b4c42-ec0d-42a1-b45a-af6b27949b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "batch_size = 52\n",
        "\n",
        "context_mean = [0.4690646, 0.4407227, 0.40508908]\n",
        "context_std = [0.2514227, 0.24312855, 0.24266963]\n",
        "body_mean = [0.43832874, 0.3964344, 0.3706214]\n",
        "body_std = [0.24784276, 0.23621225, 0.2323653]\n",
        "context_norm = [context_mean, context_std]\n",
        "body_norm = [body_mean, body_std]\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([transforms.ToPILImage(),transforms.RandomHorizontalFlip(), transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4), transforms.ToTensor()])\n",
        "test_transform = transforms.Compose([transforms.ToPILImage(),transforms.ToTensor()])\n",
        "\n",
        "train_dataset = Emotic_PreDataset(train_context, train_body, train_cat, train_cont, train_transform, context_norm, body_norm)\n",
        "val_dataset = Emotic_PreDataset(val_context, val_body, val_cat, val_cont, test_transform, context_norm, body_norm)\n",
        "test_dataset = Emotic_PreDataset(test_context, test_body, test_cat, test_cont, test_transform, context_norm, body_norm)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size, shuffle=False) \n",
        "\n",
        "print ('train loader ', len(train_loader), 'val loader ', len(val_loader), 'test', len(test_loader))\n",
        "print ('completed cell')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loader  447 val loader  64 test 139\n",
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvPoFnAliZBC",
        "colab_type": "text"
      },
      "source": [
        "# V. Prepare emotic model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMSaPqJyVyEW",
        "colab_type": "code",
        "outputId": "6da4eb89-e1b8-4891-c8fe-aab416bebeb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_path_places = './places'\n",
        "model_context = torch.load(os.path.join(model_path_places,'res_context.pth'))\n",
        "model_body = models.resnet18(pretrained=True)\n",
        "\n",
        "emotic_model = Emotic(list(model_context.children())[-1].in_features, list(model_body.children())[-1].in_features)\n",
        "model_context = nn.Sequential(*(list(model_context.children())[:-1]))\n",
        "model_body = nn.Sequential(*(list(model_body.children())[:-1]))\n",
        "\n",
        "\n",
        "# print (summary(model_context, (3,224,224), device=\"cpu\"))\n",
        "# print (summary(model_body, (3,128,128), device=\"cpu\"))\n",
        "\n",
        "print ('completed cell')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE5qh_ljPOqs",
        "colab_type": "text"
      },
      "source": [
        "## Prepare optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6-3FTclWAGh",
        "colab_type": "code",
        "outputId": "134ffac3-76f7-4192-ed51-1a538e17673f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for param in emotic_model.parameters():\n",
        "  param.requires_grad = True\n",
        "for param in model_context.parameters():\n",
        "  param.requires_grad = False\n",
        "for param in model_body.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "opt = optim.Adam((list(emotic_model.parameters()) + list(model_context.parameters()) + list(model_body.parameters())), lr=0.001, weight_decay=5e-4)\n",
        "scheduler = StepLR(opt, step_size=7, gamma=0.1)\n",
        "\n",
        "disc_loss = DiscreteLoss('dynamic', device)\n",
        "cont_loss_SL1 = ContinuousLoss_SL1()\n",
        "\n",
        "print ('completed cell')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvUH2QxGjCEc",
        "colab_type": "text"
      },
      "source": [
        "# VI. Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqtB3MrzA3Uj",
        "colab_type": "code",
        "outputId": "d1bbf51a-76ee-4d13-97d0-2bf22108d660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def train_emotic(epochs, model_path, opt, scheduler, models, disc_loss, cont_loss, cat_loss_param=0.5, cont_loss_param=0.5):\n",
        "  if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "  \n",
        "  train_loss = list()\n",
        "  val_loss = list()\n",
        "\n",
        "  model_context, model_body, emotic_model = models\n",
        "  emotic_model.to(device)\n",
        "  model_context.to(device)\n",
        "  model_body.to(device)\n",
        "\n",
        "  for e in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    emotic_model.train()\n",
        "    model_context.train()\n",
        "    model_body.train()\n",
        "    \n",
        "    for images_context, images_body, labels_cat, labels_cont in iter(train_loader):\n",
        "      images_context = images_context.to(device)\n",
        "      images_body = images_body.to(device)\n",
        "      labels_cat = labels_cat.to(device)\n",
        "      labels_cont = labels_cont.to(device)\n",
        "\n",
        "      opt.zero_grad()\n",
        "\n",
        "      pred_context = model_context(images_context)\n",
        "      pred_body = model_body(images_body)\n",
        "\n",
        "      pred_cat, pred_cont = emotic_model(pred_context, pred_body)\n",
        "      cat_loss_batch = disc_loss(pred_cat, labels_cat)\n",
        "      cont_loss_batch = cont_loss(pred_cont * 10, labels_cont * 10)\n",
        "      loss = (cat_loss_param * cat_loss_batch) + (cont_loss_param * cont_loss_batch)\n",
        "      running_loss += loss.item()\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "    if e % 1 == 0: \n",
        "      print ('epoch = %d training loss = %.4f' %(e, running_loss))\n",
        "    train_loss.append(running_loss)\n",
        "\n",
        "    \n",
        "    running_loss = 0.0 \n",
        "    emotic_model.eval()\n",
        "    model_context.eval()\n",
        "    model_body.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for images_context, images_body, labels_cat, labels_cont in iter(val_loader):\n",
        "        images_context = images_context.to(device)\n",
        "        images_body = images_body.to(device)\n",
        "        labels_cat = labels_cat.to(device)\n",
        "        labels_cont = labels_cont.to(device)\n",
        "\n",
        "        pred_context = model_context(images_context)\n",
        "        pred_body = model_body(images_body)\n",
        "        \n",
        "        pred_cat, pred_cont = emotic_model(pred_context, pred_body)\n",
        "        cat_loss_batch = disc_loss(pred_cat, labels_cat)\n",
        "        cont_loss_batch = cont_loss(pred_cont * 10, labels_cont * 10)\n",
        "        loss = (cat_loss_param * cat_loss_batch) + (cont_loss_param * cont_loss_batch)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "      if e % 1 == 0:\n",
        "        print ('epoch = %d validation loss = %.4f' %(e, running_loss))\n",
        "    val_loss.append(running_loss)\n",
        "      \n",
        "    scheduler.step()\n",
        "  \n",
        "  print ('completed training')\n",
        "  emotic_model.to(\"cpu\")\n",
        "  model_context.to(\"cpu\")\n",
        "  model_body.to(\"cpu\")\n",
        "  torch.save(emotic_model, os.path.join(model_path, 'model_emotic.pth'))\n",
        "  torch.save(model_context, os.path.join(model_path, 'model_context.pth'))\n",
        "  torch.save(model_body, os.path.join(model_path, 'model_body.pth'))\n",
        "  \n",
        "  f, (ax1, ax2) = plt.subplots(1, 2, figsize = (6, 6))\n",
        "  f.suptitle('emotic')\n",
        "  ax1.plot(range(0,len(train_loss)),train_loss, color='Blue')\n",
        "  ax2.plot(range(0,len(val_loss)),val_loss, color='Red')\n",
        "  ax1.legend(['train'])\n",
        "  ax2.legend(['val'])\n",
        "\n",
        "print ('completed cell')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1KsKv_hwoUC",
        "colab_type": "code",
        "outputId": "d2b1b6fd-fb90-46c6-815e-2d76e111c07e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "train_emotic(15, './models', opt, scheduler, [model_context, model_body, emotic_model], disc_loss, cont_loss_SL1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch = 0 training loss = 71042.0836\n",
            "epoch = 0 validation loss = 5610.5711\n",
            "epoch = 1 training loss = 46261.7234\n",
            "epoch = 1 validation loss = 5833.7489\n",
            "epoch = 2 training loss = 44434.1976\n",
            "epoch = 2 validation loss = 5381.3547\n",
            "epoch = 3 training loss = 43240.4571\n",
            "epoch = 3 validation loss = 5336.5164\n",
            "epoch = 4 training loss = 42406.0807\n",
            "epoch = 4 validation loss = 5020.6323\n",
            "epoch = 5 training loss = 41881.0489\n",
            "epoch = 5 validation loss = 4966.8540\n",
            "epoch = 6 training loss = 41308.0901\n",
            "epoch = 6 validation loss = 4995.1876\n",
            "epoch = 7 training loss = 40125.8464\n",
            "epoch = 7 validation loss = 4894.0163\n",
            "epoch = 8 training loss = 39686.4472\n",
            "epoch = 8 validation loss = 4853.1592\n",
            "epoch = 9 training loss = 39558.2041\n",
            "epoch = 9 validation loss = 4923.9077\n",
            "epoch = 10 training loss = 39413.6790\n",
            "epoch = 10 validation loss = 4849.4048\n",
            "epoch = 11 training loss = 39368.5059\n",
            "epoch = 11 validation loss = 4826.8070\n",
            "epoch = 12 training loss = 39316.5346\n",
            "epoch = 12 validation loss = 4815.0648\n",
            "epoch = 13 training loss = 39161.4047\n",
            "epoch = 13 validation loss = 4855.4562\n",
            "epoch = 14 training loss = 39052.7824\n",
            "epoch = 14 validation loss = 4883.1666\n",
            "completed training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Emotic. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDa4nuQvjGSa",
        "colab_type": "text"
      },
      "source": [
        "# VII. Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFCcFv4mnmRi",
        "colab_type": "code",
        "outputId": "cf3ff83f-7176-422a-ebbe-a0d82d77607e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def test_scikit_ap(cat_preds, cat_labels):\n",
        "  ap = np.zeros(26, dtype=np.float32)\n",
        "  for i in range(26):\n",
        "    ap[i] = average_precision_score(cat_labels[i, :], cat_preds[i, :])\n",
        "  print ('ap', ap, ap.shape, ap.mean())\n",
        "  return ap.mean()\n",
        "\n",
        "def test_emotic_vad(cont_preds, cont_labels):\n",
        "  vad = np.zeros(3, dtype=np.float32)\n",
        "  for i in range(3):\n",
        "    vad[i] = np.mean(np.abs(cont_preds[i, :] - cont_labels[i, :]))\n",
        "  print ('vad', vad, vad.shape, vad.mean())\n",
        "  return vad.mean()\n",
        "\n",
        "def get_thresholds(cat_preds, cat_labels):\n",
        "  thresholds = np.zeros(26, dtype=np.float32)\n",
        "  for i in range(26):\n",
        "    p, r, t = precision_recall_curve(cat_labels[i, :], cat_preds[i, :])\n",
        "    for k in range(len(p)):\n",
        "      if p[k] == r[k]:\n",
        "        thresholds[i] = t[k]\n",
        "        break\n",
        "  np.save('./thresholds.npy', thresholds)\n",
        "  return thresholds\n",
        "\n",
        "print ('completed cell')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KOeZRVdbUPNx",
        "outputId": "dfecf880-f7ed-4bb1-c355-f07ce839b899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def test_data(models, device, data_loader, num_images):\n",
        "    model_context, model_body, emotic_model = models\n",
        "    cat_preds = np.zeros((num_images, 26))\n",
        "    cat_labels = np.zeros((num_images, 26))\n",
        "    cont_preds = np.zeros((num_images, 3))\n",
        "    cont_labels = np.zeros((num_images, 3))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model_context.to(device)\n",
        "        model_body.to(device)\n",
        "        emotic_model.to(device)\n",
        "        model_context.eval()\n",
        "        model_body.eval()\n",
        "        emotic_model.eval()\n",
        "        indx = 0\n",
        "        print ('starting testing')\n",
        "        for images_context, images_body, labels_cat, labels_cont in iter(data_loader):\n",
        "            images_context = images_context.to(device)\n",
        "            images_body = images_body.to(device)\n",
        "\n",
        "            pred_context = model_context(images_context)\n",
        "            pred_body = model_body(images_body)\n",
        "            pred_cat, pred_cont = emotic_model(pred_context, pred_body)\n",
        "\n",
        "            cat_preds[ indx : (indx + pred_cat.shape[0]), :] = pred_cat.to(\"cpu\").data.numpy()\n",
        "            cat_labels[ indx : (indx + labels_cat.shape[0]), :] = labels_cat.to(\"cpu\").data.numpy()\n",
        "            cont_preds[ indx : (indx + pred_cont.shape[0]), :] = pred_cont.to(\"cpu\").data.numpy() * 10\n",
        "            cont_labels[ indx : (indx + labels_cont.shape[0]), :] = labels_cont.to(\"cpu\").data.numpy() * 10 \n",
        "            indx = indx + pred_cat.shape[0]\n",
        "\n",
        "    cat_preds = cat_preds.transpose()\n",
        "    cat_labels = cat_labels.transpose()\n",
        "    cont_preds = cont_preds.transpose()\n",
        "    cont_labels = cont_labels.transpose()\n",
        "    scipy.io.savemat('./cat_preds.mat',mdict={'cat_preds':cat_preds})\n",
        "    scipy.io.savemat('./cat_labels.mat',mdict={'cat_labels':cat_labels})\n",
        "    scipy.io.savemat('./cont_preds.mat',mdict={'cont_preds':cont_preds})\n",
        "    scipy.io.savemat('./cont_labels.mat',mdict={'cont_labels':cont_labels})\n",
        "    print ('completed testing')\n",
        "    ap_mean = test_scikit_ap(cat_preds, cat_labels)\n",
        "    vad_mean = test_emotic_vad(cont_preds, cont_labels)\n",
        "    print (ap_mean, vad_mean)\n",
        "    return ap_mean, vad_mean \n",
        "\n",
        "print ('completed cell')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB69Xo-kLldG",
        "colab_type": "code",
        "outputId": "83d7bc40-c0b9-4dd2-e4ae-5441034c220c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "val_ap, val_vad = test_data([model_context, model_body, emotic_model], device, val_loader, val_dataset.__len__())\n",
        "test_ap, test_vad = test_data([model_context, model_body, emotic_model], device, test_loader, test_dataset.__len__())\n",
        "\n",
        "print ('validation Mean average precision=%.4f Mean VAD MAE=%.4f' %(val_ap, val_vad))\n",
        "print ('testing Mean average precision=%.4f Mean VAD MAE=%.4f' %(test_ap, test_vad))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting testing\n",
            "completed testing\n",
            "ap [0.3949865  0.17890823 0.21746121 0.95054156 0.17780074 0.7931695\n",
            " 0.229143   0.37020528 0.18187584 0.21136093 0.05526229 0.98093367\n",
            " 0.25903982 0.7995634  0.1182593  0.08988345 0.80778223 0.15500002\n",
            " 0.28862482 0.49514762 0.19118501 0.07363844 0.1871227  0.15210989\n",
            " 0.36093006 0.11644995] (26,) 0.33986098\n",
            "vad [0.7142576  0.84440917 0.9054704 ] (3,) 0.821379\n",
            "0.33986098 0.821379\n",
            "starting testing\n",
            "completed testing\n",
            "ap [0.2874401  0.08676253 0.14388956 0.5655653  0.07020731 0.75880736\n",
            " 0.11639392 0.24121834 0.16281651 0.17114119 0.01929642 0.86306214\n",
            " 0.15631099 0.6962069  0.08706686 0.05460502 0.66348666 0.06360297\n",
            " 0.21989821 0.41962627 0.17866218 0.05818626 0.17521654 0.08399871\n",
            " 0.12673652 0.07929516] (26,) 0.25190386\n",
            "vad [0.912804   1.027487   0.96414095] (3,) 0.968144\n",
            "0.25190386 0.968144\n",
            "validation Mean average precision=0.3399 Mean VAD MAE=0.8214\n",
            "testing Mean average precision=0.2519 Mean VAD MAE=0.9681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-fc5LNp4len",
        "colab_type": "code",
        "outputId": "a773fa6b-c31c-40c5-be1b-6c5e17291b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "cat_labels = scipy.io.loadmat('./cat_labels.mat')\n",
        "cat_preds = scipy.io.loadmat('./cat_preds.mat')\n",
        "cat_preds = cat_preds['cat_preds']\n",
        "cat_labels = cat_labels['cat_labels']\n",
        "print (cat_preds.shape, cat_labels.shape)\n",
        "\n",
        "#thesholds calculation for inference \n",
        "thresholds = get_thresholds(cat_preds, cat_labels)\n",
        "print (thresholds, thresholds.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26, 7203) (26, 7203)\n",
            "[0.08701557 0.1589056  0.09289873 0.19048621 0.11370537 0.13716227\n",
            " 0.12086156 0.09053159 0.04740949 0.05390422 0.05036047 0.47891623\n",
            " 0.05516443 0.16045685 0.06358694 0.05951726 0.23607145 0.11357296\n",
            " 0.09494811 0.09920253 0.09788954 0.07316053 0.12010533 0.05801693\n",
            " 0.06555955 0.05944895] (26,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owTpkHmOjLvr",
        "colab_type": "text"
      },
      "source": [
        "# VIII. Average Precision computation using <a href=\"https://1drv.ms/u/s!AkYHbdGNmIVCgbYZB_dY3wuWJou_5A?e=jcsZUj\">author's script</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30PEDPHxrkXA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "8d2ed78c-fadb-40fc-8f11-be409beb8ea0"
      },
      "source": [
        "!apt install octave"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "octave is already the newest version (4.2.2-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fWR4CTMr7Hf",
        "colab_type": "code",
        "outputId": "b7539f27-3a07-4184-f67f-d4b3d84350f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile eval.m\n",
        "\n",
        "gt = load('./cat_labels.mat')\n",
        "gt = gt.cat_labels\n",
        "\n",
        "pred = load('./cat_preds.mat')\n",
        "pred = pred.cat_preds\n",
        "\n",
        "categories{1} = 'Affection';\n",
        "categories{2} = 'Anger';\n",
        "categories{3} = 'Annoyance';\n",
        "categories{4} = 'Anticipation';\n",
        "categories{5} = 'Aversion';\n",
        "categories{6} = 'Confidence';\n",
        "categories{7} = 'Disapproval';\n",
        "categories{8} = 'Disconnection';\n",
        "categories{9} = 'Disquietment';\n",
        "categories{10} = 'Doubt/Confusion';\n",
        "categories{11} = 'Embarrassment';\n",
        "categories{12} = 'Engagement';\n",
        "categories{13} = 'Esteem';\n",
        "categories{14} = 'Excitement';\n",
        "categories{15} = 'Fatigue';\n",
        "categories{16} = 'Fear';\n",
        "categories{17} = 'Happiness';\n",
        "categories{18} = 'Pain';\n",
        "categories{19} = 'Peace';\n",
        "categories{20} = 'Pleasure';\n",
        "categories{21} = 'Sadness';\n",
        "categories{22} = 'Sensitivity';\n",
        "categories{23} = 'Suffering';\n",
        "categories{24} = 'Surprise';\n",
        "categories{25} = 'Sympathy';\n",
        "categories{26} = 'Yearning';\n",
        "\n",
        "\n",
        "for c = 1:length(categories)\n",
        "  confidence = pred(c,:)'; \n",
        "  testClass = gt(c,:)';\n",
        "  confidence = double(confidence);\n",
        "\n",
        "  S = rand('state');\n",
        "  rand('state',0);\n",
        "  confidence = confidence + rand(size(confidence))*10^(-10);\n",
        "  rand('state',S)\n",
        "\n",
        "  [S,j] = sort(-confidence);\n",
        "  C = testClass(j);\n",
        "  n = length(C);\n",
        "    \n",
        "  REL = sum(C);\n",
        "  if n>0\n",
        "    RETREL = cumsum(C);\n",
        "    RET    = (1:n)';\n",
        "  else\n",
        "    RETREL = 0;\n",
        "    RET    = 1;\n",
        "  end\n",
        "\n",
        "  precision = 100*RETREL ./ RET;\n",
        "  recall    = 100*RETREL  / REL;\n",
        "  th = -S;\n",
        "\n",
        "  % compute AP\n",
        "  mrec=[0 ; recall ; 100];\n",
        "  mpre=[0 ; precision ; 0];\n",
        "  for i=numel(mpre)-1:-1:1\n",
        "    mpre(i)=max(mpre(i),mpre(i+1));\n",
        "  end\n",
        "  i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
        "  averagePrecision=sum((mrec(i)-mrec(i-1)).*mpre(i))/100;\n",
        "  ap_list(c)  = averagePrecision\n",
        "end\n",
        "\n",
        "display('#######################################')\n",
        "\n",
        "display('Average precision of predictions');\n",
        "for c = 1:length(categories)\n",
        "    sp = '............................';\n",
        "    cat = strcat(categories{c}, sp);\n",
        "    cat = cat(1:18);\n",
        "    display(cat);\n",
        "    display(ap_list(c));\n",
        "end"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting eval.m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA1Oc48zvI_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!octave -W eval.m"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}